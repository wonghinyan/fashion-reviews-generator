{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fashion review generator\n",
    "\n",
    "In this project, I am going to use product reviews written by genuine shoppers and generate new product reviews copy. The purpose is to demonstrate the usefulness of RNN and whether they can be used to create advertising copy for ecommerce. The data set comes from:\n",
    "https://www.kaggle.com/nicapotato/womens-ecommerce-clothing-reviews (requires Kaggle login).\n",
    "\n",
    "This is a genuine product review: \n",
    "\n",
    ">_Snap this one up but make sure you order a size larger than your normal size. it runs a full size too small. it sold out before and i was told it wasn't coming back in. i had purchased one previously in my regular size but it was too small. to my delight, it's back, and i've purchased one in the next size up. it's well made, can go from business to sport to casual._\n",
    "\n",
    "This is a review generated by our RNN: \n",
    "\n",
    ">_this is the right color and i am a little small. the fabric is soft and the dress was very pretty and the fabric looks great on me._\n",
    "\n",
    "To make the fashion review generator, we take the following steps:\n",
    "- Get and load the data.\n",
    "- Create a vocabulary look up table based on the complete set of reviews. This transforms words into integers (which our RNN can be trained on), and a reverse table to turn integers back into words.\n",
    "- Tokenize punctuation which removes duplicates (e.g. good and good!).  \n",
    "- Pre-process the product reviews by creating a vocabulary look up table, replacing the punctuation with tokens, converting text to lower case, split them up and turning each word into an integer from the look up table.\n",
    "- Batching the data ready for training.\n",
    "- Build the recurrent neural network / LTSM.\n",
    "- Define forward pass and back propagation.\n",
    "- Train the RNN with hyper parameter tuning to achieve desired level of performance.\n",
    "- Save check point.\n",
    "- Generate new reviews by supplying a small number of words. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load libraries\n",
    "Loading Python libraries required for our processing the data, defining and training our RNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load file operation libraries\n",
    "import os\n",
    "import pickle\n",
    "import csv\n",
    "\n",
    "# Load data manipulation libraries\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "# Load PyTorch libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check availability of GPU\n",
    "RNN takes ages to train on a CPU. So it is best to train our network on a GPU enabled machine. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on GPU.\n"
     ]
    }
   ],
   "source": [
    "# Check for a GPU\n",
    "train_on_gpu = torch.cuda.is_available()\n",
    "if train_on_gpu:\n",
    "    print('Training on GPU.')\n",
    "else:\n",
    "    print('No GPU found: using CPU instead.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the data\n",
    "\n",
    "We download and unzip the data file from Kaggle and place it in the `data` folder. The CSV file `Womens Clothing E-Commerce Reviews.csv` is in the data directory. We use `csv` to read the reviews and store in `fashion_reviews` for processing later. Now `fashion_reviews` only contains a list of reviews, their rating (5 point scale) and the recommendation (1 = recommend, 0 = not recommended). Since we are only interested in the reviews where a purchase is recommended by the shopper, we discard the non-recommendations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the CSV file and only import the columns containing the review copy, \n",
    "# 5 point scale and recommendation\n",
    "\n",
    "fashion_reviews = []\n",
    "with open('data/Womens Clothing E-Commerce Reviews.csv') as csvfile:\n",
    "    review_reader = csv.reader(csvfile, delimiter = ',', quotechar = '\"')\n",
    "    for row in review_reader:\n",
    "        fashion_reviews.append(row[4:7])\n",
    "    # Remove the header (first) row which we won't use\n",
    "    fashion_reviews.pop(0)\n",
    "\n",
    "# Store \"recommended\" reviews in recommended_reviews\n",
    "recommended_reviews = []\n",
    "for row in fashion_reviews:\n",
    "    if row[2] == '1':\n",
    "        recommended_reviews.append(row[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore the data\n",
    "\n",
    "Taking a look at how many reviews there are, and print out a random sample of the reviews. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 23486 reviews in total, of which 19314 recommend a particular product.\n",
      "Average number of words in each recommended review is 57.40100445272859\n",
      "\n",
      "Here is a sample of reviews:\n",
      "\n",
      "Review 13978 is: The dress is beautiful, i wanted to like it and could hardly wait to reveive it. it fits beautifully, and snug except around the arms where it is huge. i could only wear it with a tshirt underneat.\n",
      "\n",
      "Review 13637 is: This dress is absolutely gorgeous! the top portion fit a little smaller than the typical size i order. the zip side was a little snug, but the fit is very flattering! the lace detail and belted design are very pretty. the skirt portion has more volume than i tend to like, but it is it beautiful!\n",
      "\n",
      "Review 18835 is: I ordered this online and i love the style, but didn't like the print as much as i thought i would.\n",
      "\n",
      "Review 11064 is: I love this dress. the fabric is butter soft, the strapless top stays in place and it hangs beautifully. it is such a wonderfully comfortable summer dress. as previously stated, i'm in love.\n",
      "\n",
      "Review 10250 is: I bought the ivory sweater. lovely color and softness. the zipper is a great, unique feature that i love. the bottom has a wavy shape with extra fabric - definitely creates a bell shape.\n"
     ]
    }
   ],
   "source": [
    "# Determine number of reviews\n",
    "n_reviews = len(fashion_reviews)\n",
    "n_recommended_reviews = len(recommended_reviews)\n",
    "print('There are {} reviews in total, of which {} recommend a particular product.'.format(n_reviews,\n",
    "                                                                                          n_recommended_reviews))\n",
    "\n",
    "# Calculate average number of words per review\n",
    "word_count_row = [len(row.split()) for row in recommended_reviews]\n",
    "print('Average number of words in each recommended review is', np.average(word_count_row))\n",
    "\n",
    "# Show a sample selection of reviews\n",
    "print(\"\\nHere is a sample of reviews:\")\n",
    "for idx in range(5):\n",
    "    i_review = np.random.randint(0, n_recommended_reviews)\n",
    "    print(\"\\nReview {} is: {}\".format(i_review, recommended_reviews[i_review]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create pre-processing functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since a neural network cannot \"read\" the text in the reviews, we need to convert the text into integers that our neural network can learn from. We first create a lookup table for all the words in the reviews, then we create a reverse lookup table for when we can generate new reviews. Punctuations form an integral part of any reviews. Just like words, a neural network cannot read them. Here we will replace punctuations in the reviews with a token. \n",
    "\n",
    "Unlike sentiment analysis, we are including all stop words (e.g. 'to', 'the') and do not make a distinction of words that have the same stem (e.g. 'beauty', 'beautiful', 'beatifully')."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function creates a lookup table of all the words used in the review. \n",
    "def create_lookup_tables(reviews):\n",
    "    # Use Counter to count the words\n",
    "    counted_words = Counter(reviews)\n",
    "    \n",
    "    # Sort the words in decending order of occurance\n",
    "    sorted_vocab = sorted(counted_words, key = counted_words.get, reverse = True)\n",
    "\n",
    "    # Create int_to_vocab dictinoaries\n",
    "    int_to_vocab = {idx: word for idx, word in enumerate(sorted_vocab)}\n",
    "    vocab_to_int = {word: idx for idx, word in int_to_vocab.items()}\n",
    "    \n",
    "    return vocab_to_int, int_to_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function returns a dictionary of punctuation tokens that will be used to \n",
    "# replace punctuations in reviews with a token. \n",
    "def token_lookup():    \n",
    "    tokens_dict = {'.':'<FULLSTOP>',\n",
    "                   ',':'<COMMA>',\n",
    "                   '?':'<QUESTION_MARK>',\n",
    "                   '!':'<EXCLAMATION_MARK>',\n",
    "                   '\"':'<QUOTATION_MARK>',\n",
    "                   ';':'<SEMICOLON>',\n",
    "                   '-':'<DASH>',\n",
    "                   '(':'<LEFT_PAREN>',\n",
    "                   ')':'<RIGHT_PAREN>',\n",
    "                   '\\n':'<CARRIAGE_RETURN>'}\n",
    "    return tokens_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These two functions saves the pre processed data, and load them\n",
    "# back at a subsequent session.\n",
    "\n",
    "padding_words = {'PADDING': '<PAD>'}\n",
    "\n",
    "def preprocess_and_save_data(reviews, token_lookup, create_lookup_tables, filename):\n",
    "    # The reviews are in a list, and they need to be converted into \n",
    "    # a single piece of text.\n",
    "    reviews = \"\".join(reviews)\n",
    "    \n",
    "    # Create the punctuation token lookup and replace all punctuations\n",
    "    # in the reviews with them.\n",
    "    token_dict = token_lookup()\n",
    "    for key, token in token_dict.items():\n",
    "        reviews = reviews.replace(key, ' {} '.format(token)) \n",
    "    # The additional spaces are essential to separate punctuations from words\n",
    "\n",
    "    # Covert allwords to lower case, then split them into individual\n",
    "    # words to form a tuple.\n",
    "    reviews = reviews.lower()\n",
    "    reviews = reviews.split()\n",
    "\n",
    "    vocab_to_int, int_to_vocab = create_lookup_tables(reviews + list(padding_words.values()))\n",
    "    # int_review contains integer-ized version of the aggregated reviews\n",
    "    int_reviews = [vocab_to_int[word] for word in reviews]\n",
    "    \n",
    "    pickle.dump((int_reviews, vocab_to_int, int_to_vocab, token_dict), \n",
    "                open(filename, 'wb'))\n",
    "    \n",
    "def load_preprocess(filename):\n",
    "    # Load the preprocessed data (just saved) and return them in batches\n",
    "    return pickle.load(open(filename, mode='rb'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now running the pre-processing and save the data \n",
    "preprocess_filename = 'preprocess.p'\n",
    "preprocess_and_save_data(recommended_reviews, token_lookup, create_lookup_tables, preprocess_filename)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load processed data and check point ###\n",
    "\n",
    "We call the `load_preprocess` function to load the pre-processed data. Use this function to re-load the pre-processed data without re-processing the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[254, 491, 19, 876, 3, 613, 3, 6341, 8, 20, 11, 35, 1812, 105, 0, 2, 1623, 9, 208, 6, 10, 5, 106, 4, 3, 40, 287, 2, 145, 1336, 2, 425, 53, 27, 64, 6, 149, 1336, 35, 98, 0, 2, 72, 5, 98, 3, 41, 509, 39, 0]\n"
     ]
    }
   ],
   "source": [
    "### CHECK POINT ###\n",
    "padding_words = {'PADDING': '<PAD>'} # this value is used later\n",
    "\n",
    "preprocess_filename = 'preprocess.p'\n",
    "int_reviews, vocab_to_int, int_to_vocab, token_dict = load_preprocess(preprocess_filename)\n",
    "\n",
    "print(int_reviews[:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n"
     ]
    }
   ],
   "source": [
    "print(vocab_to_int['dress'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch the data for training and define the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch the data for training\n",
    "\n",
    "Our RNN will train on data in batches. The `batch_data` function below will take in a sequnece of pre-processed words and batch them into pre-specified lengths. Batching also defines the length of a sequence of words (`sequence_length`) and the target word.\n",
    "\n",
    "For example, if we have a sentence \n",
    ">this is a very nice dress and the color suits me.\n",
    "\n",
    "and `sequence_length = 5`, then\n",
    "\n",
    "Features: [8, 7, 5, 26, 75] for 'this is a very nice'\n",
    "\n",
    "Target: 20 for 'dress'\n",
    "\n",
    "This function will go through the tokenized review provided, and turn them into batches of features and target tensors using PyTorch's `DataLoader`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_data(words, sequence_length, batch_size):\n",
    "    # Define number of batches\n",
    "    n_batches = len(words)//batch_size\n",
    "    # ... and discard orphan words of incomplete batches\n",
    "    words = words[:n_batches * batch_size]\n",
    "    # print('words:', words)\n",
    "    \n",
    "    # Define feature_tensors and target_tensors \n",
    "    feature_tensors, target_tensors = [], []\n",
    "    \n",
    "    # Going through words in the reviews, one batch (batch_size as step) at a time\n",
    "    # For each batch of the total number of batches\n",
    "    for idx_batch in range(0, len(words), batch_size):\n",
    "    \n",
    "        this_batch = words[idx_batch : idx_batch + batch_size + sequence_length]\n",
    "        for idx_word in range(batch_size):\n",
    "            if batch_size - idx_word >= sequence_length + 1:                \n",
    "                # get feature\n",
    "                batch_feature = this_batch[idx_word:idx_word + sequence_length]\n",
    "                # print('{} batch_feature: {}'.format(idx_word, batch_feature))\n",
    "                feature_tensors.append(batch_feature)\n",
    "                \n",
    "                # get target which is the following (sequence_length) set of words\n",
    "                batch_target = this_batch[idx_word + sequence_length]\n",
    "                # print('{} batch_target: {}'.format(idx_word + sequence_length, batch_target))\n",
    "                target_tensors.append(batch_target)\n",
    "                \n",
    "            else:\n",
    "                pass\n",
    "\n",
    "    feature_tensors = torch.from_numpy(np.asarray(feature_tensors))\n",
    "    feature_tensors = torch.LongTensor(feature_tensors)\n",
    "    print('\\nfeature_tensors size:', feature_tensors.size())\n",
    "    \n",
    "    target_tensors = torch.LongTensor(target_tensors) # a LongTensor is expected in the loss function\n",
    "    print('\\ntarget_tensors size:', target_tensors.size())\n",
    "    \n",
    "    data = TensorDataset(feature_tensors, target_tensors)\n",
    "    data_loader = torch.utils.data.DataLoader(data, batch_size=batch_size, shuffle = True )\n",
    "\n",
    "    return data_loader \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the recurrent neural network (LSTM)\n",
    "\n",
    "We are implementing a RNN using PyTorch's `nn.Module` class, which requires us to specify:\n",
    "- number of embedding features\n",
    "- number of hidden features\n",
    "- number of layers in the LSTM network \n",
    "- and whether the input and output tensors are batched\n",
    "\n",
    "The feed forward function passes the batched input through the LSTM layers (with dropout) and onto a fully connected layer.\n",
    "\n",
    "We also define a function to initiate the hidden features as zeroes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size, output_size, embedding_dim, hidden_dim, n_layers, dropout=0.5):\n",
    "        super(RNN, self).__init__()\n",
    "        \n",
    "        # set class variables\n",
    "        self.vocab_size = vocab_size\n",
    "        self.output_size = output_size\n",
    "        self.n_embedding = embedding_dim \n",
    "        self.n_hidden = hidden_dim\n",
    "        self.n_layers = n_layers\n",
    "        self.drop_prob = dropout\n",
    "        \n",
    "        # define model layers\n",
    "        self.embedding = nn.Embedding(self.vocab_size, self.n_embedding)\n",
    "        \n",
    "        self.lstm = nn.LSTM(self.n_embedding,\n",
    "                            self.n_hidden,\n",
    "                            self.n_layers,\n",
    "                            batch_first = True)\n",
    "        self.dropout = nn.Dropout(self.drop_prob)\n",
    "        self.fc = nn.Linear(self.n_hidden, self.vocab_size)\n",
    "    \n",
    "    def forward(self, nn_input, hidden):\n",
    "        batch_size = nn_input.size(0)\n",
    "        # Put input into embedded layer\n",
    "        embed = self.embedding(nn_input) \n",
    "        # Then onto LSTM layers\n",
    "        output, hidden = self.lstm(embed, hidden)\n",
    "        output = output.contiguous().view(-1, self.n_hidden) # stacking the LSTM\n",
    "        output = self.dropout(output)\n",
    "        # Then onto output layer\n",
    "        output = self.fc(output)\n",
    "\n",
    "        output = output.view(batch_size, -1, self.output_size) \n",
    "        output = output[:, -1] # get last batch\n",
    "\n",
    "        # return one batch of output word scores and the hidden state\n",
    "        return output, hidden\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        # Initialize the hidden state of an LSTM with zero weights,\n",
    "        # and move to GPU if available\n",
    "        \n",
    "        weight = next(self.parameters()).data\n",
    "        if train_on_gpu:\n",
    "            hidden = (weight.new(self.n_layers, batch_size, self.n_hidden).zero_().cuda(),\n",
    "                      weight.new(self.n_layers, batch_size, self.n_hidden).zero_().cuda())\n",
    "        else:\n",
    "            hidden = (weight.new(self.n_layers, batch_size, self.n_hidden).zero_(),\n",
    "                      weight.new(self.n_layers, batch_size, self.n_hidden).zero_())\n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the forward and backpropagation functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_back_prop(rnn, optimizer, criterion, input_data, target, hidden):\n",
    "    \n",
    "    if train_on_gpu:\n",
    "        input_data, target = input_data.cuda(), target.cuda()\n",
    "    \n",
    "    hidden = tuple([each.data for each in hidden])\n",
    "    \n",
    "    rnn.zero_grad()\n",
    "    \n",
    "    # Perform forward pass\n",
    "    output, hidden = rnn(input_data, hidden)\n",
    "    \n",
    "    # Perform backpropagation\n",
    "    loss = criterion(output, target)\n",
    "    loss.backward()\n",
    "    \n",
    "    # Avoid gradient explosion through clipping\n",
    "    nn.utils.clip_grad_norm_(rnn.parameters(), max_norm = 5) # max_norm = 5 seems to be the convention\n",
    "    optimizer.step()    \n",
    "    \n",
    "    # Calculate loss\n",
    "    this_loss = loss.item()\n",
    "\n",
    "    # Return the loss over a batch and the hidden state produced by our model\n",
    "    return this_loss, hidden\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the training function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_rnn(rnn, batch_size, optimizer, criterion, n_epochs, show_every_n_batches=100):\n",
    "    \n",
    "    batch_losses = []\n",
    "    \n",
    "    # Putting the model in training mode\n",
    "    rnn.train()\n",
    "\n",
    "    print(\"Training for %d epoch(s)...\" % n_epochs)\n",
    "    for epoch_i in range(1, n_epochs + 1):\n",
    "        \n",
    "        # initialize hidden state\n",
    "        hidden = rnn.init_hidden(batch_size)\n",
    "        \n",
    "        for batch_i, (inputs, labels) in enumerate(train_loader, 1):\n",
    "            \n",
    "            # This ensures we iterate over completely full batches only\n",
    "            n_batches = len(train_loader.dataset)//batch_size\n",
    "            if(batch_i > n_batches):\n",
    "                break\n",
    "            \n",
    "            # Feed forward then backpropagation\n",
    "            loss, hidden = forward_back_prop(rnn, optimizer, criterion, inputs, labels, hidden)          \n",
    "            # Record loss\n",
    "            batch_losses.append(loss)\n",
    "\n",
    "            # Print loss stats as we train\n",
    "            if batch_i % show_every_n_batches == 0:\n",
    "                print('Epoch: {:>4}/{:<4}  Loss: {}'.format(\n",
    "                    epoch_i, n_epochs, np.average(batch_losses)))\n",
    "                batch_losses = []\n",
    "\n",
    "    # returns a trained model\n",
    "    return rnn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting upt the data loader and training parameters\n",
    "\n",
    "The `DataLoader` will use the specified sequence_length and batch size. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "feature_tensors size: torch.Size([951375, 25])\n",
      "\n",
      "target_tensors size: torch.Size([951375])\n"
     ]
    }
   ],
   "source": [
    "# Setting up the data loader for training\n",
    "sequence_length = 25 # Setting to a sequence length of 25 (roughly the length of a sentence)\n",
    "batch_size = 100 # Batch Size\n",
    "train_loader = batch_data(int_reviews, sequence_length, batch_size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### As for the training parameters, we use the following:\n",
    "- Epochs 3, which will help us reach a loss of about 3.75, sufficient for our purpose.\n",
    "- Learn rate is 0.001 which is commonly recommended in LSTM literature\n",
    "- Vocabluary size is already calculated while pre-processing the data, and output size is the same.\n",
    "- Embedding dimension and hidden dimension are set to a fraction of the vocabulary size, so that the network complexity reflects the number of unique words it has to deal with. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab_size is 17487, embedding_dim is 437 by setting latter to 0.025 of former.\n",
      "vocab_size is 17487, hidden_dim is 437 by setting latter to 0.025 of former.\n"
     ]
    }
   ],
   "source": [
    "# Setting up taining parameters\n",
    "num_epochs = 3\n",
    "learning_rate = 0.001\n",
    "vocab_size = len(vocab_to_int) # Vocab size\n",
    "output_size = vocab_size # Output size\n",
    "fraction = 0.025\n",
    "\n",
    "# Embedding dimension\n",
    "embedding_dim = int(vocab_size * fraction) # setting this to 5% of the vocab size\n",
    "print('vocab_size is {}, embedding_dim is {} by setting latter to {} of former.'.format(vocab_size,\n",
    "                                                                                        embedding_dim,\n",
    "                                                                                        fraction))\n",
    "# Hidden dimension\n",
    "hidden_dim = int(vocab_size * fraction)\n",
    "print('vocab_size is {}, hidden_dim is {} by setting latter to {} of former.'.format(vocab_size,\n",
    "                                                                                        embedding_dim,\n",
    "                                                                                        fraction))\n",
    "\n",
    "# Number of RNN Layers\n",
    "n_layers = 2\n",
    "\n",
    "# Show stats for every n number of batches\n",
    "show_every_n_batches = 500"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start training\n",
    "We now start training our network with the pre-processed data (already in `DataLoader`) by specifying the optimizer (in this case Adam) and criterion (CrossEntrophyLoss). We are aiming to achieve loss of around 3.5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 3 epoch(s)...\n",
      "Epoch:    1/3     Loss: 5.890889410972595\n",
      "Epoch:    1/3     Loss: 5.187287108421326\n",
      "Epoch:    1/3     Loss: 4.97631830072403\n",
      "Epoch:    1/3     Loss: 4.820603412628174\n",
      "Epoch:    1/3     Loss: 4.743888828754425\n",
      "Epoch:    1/3     Loss: 4.66372947883606\n",
      "Epoch:    1/3     Loss: 4.597059538364411\n",
      "Epoch:    1/3     Loss: 4.529038254737854\n",
      "Epoch:    1/3     Loss: 4.485850398540497\n",
      "Epoch:    1/3     Loss: 4.443445352077484\n",
      "Epoch:    1/3     Loss: 4.4339104485511776\n",
      "Epoch:    1/3     Loss: 4.392889785766601\n",
      "Epoch:    1/3     Loss: 4.3864826965332036\n",
      "Epoch:    1/3     Loss: 4.362711019992829\n",
      "Epoch:    1/3     Loss: 4.3401008892059325\n",
      "Epoch:    1/3     Loss: 4.313764045238495\n",
      "Epoch:    1/3     Loss: 4.283340344905853\n",
      "Epoch:    1/3     Loss: 4.274296568393707\n",
      "Epoch:    1/3     Loss: 4.240500148773194\n",
      "Epoch:    2/3     Loss: 4.138813294397693\n",
      "Epoch:    2/3     Loss: 4.1313345084190365\n",
      "Epoch:    2/3     Loss: 4.126631410598755\n",
      "Epoch:    2/3     Loss: 4.132560437679291\n",
      "Epoch:    2/3     Loss: 4.117422942638397\n",
      "Epoch:    2/3     Loss: 4.082606617450714\n",
      "Epoch:    2/3     Loss: 4.105457752704621\n",
      "Epoch:    2/3     Loss: 4.072645105361938\n",
      "Epoch:    2/3     Loss: 4.069239058017731\n",
      "Epoch:    2/3     Loss: 4.07353244638443\n",
      "Epoch:    3/3     Loss: 3.8712258752558895\n",
      "Epoch:    3/3     Loss: 3.8995427927970887\n",
      "Epoch:    3/3     Loss: 3.8876529560089113\n",
      "Epoch:    3/3     Loss: 3.9083991866111756\n",
      "Epoch:    3/3     Loss: 3.919819601535797\n",
      "Epoch:    3/3     Loss: 3.9001548709869387\n",
      "Epoch:    3/3     Loss: 3.883032787322998\n",
      "Epoch:    3/3     Loss: 3.89574094247818\n",
      "Epoch:    3/3     Loss: 3.914816185474396\n",
      "Epoch:    3/3     Loss: 3.947294843196869\n",
      "Epoch:    3/3     Loss: 3.9188100337982177\n",
      "Epoch:    3/3     Loss: 3.919390476226807\n",
      "Epoch:    3/3     Loss: 3.929768030166626\n",
      "Epoch:    3/3     Loss: 3.9050772719383238\n",
      "Epoch:    3/3     Loss: 3.9168003125190736\n",
      "Epoch:    3/3     Loss: 3.9011031703948973\n",
      "Epoch:    3/3     Loss: 3.916726954460144\n",
      "Epoch:    3/3     Loss: 3.9129031252861024\n",
      "Epoch:    3/3     Loss: 3.9399222407341004\n"
     ]
    }
   ],
   "source": [
    "# Create model and move to GPU if available\n",
    "this_RNN = RNN(vocab_size, output_size, embedding_dim, hidden_dim, n_layers, dropout=0.5)\n",
    "\n",
    "if train_on_gpu:\n",
    "    this_RNN.cuda()\n",
    "\n",
    "# Defining loss and optimization functions for training\n",
    "optimizer = torch.optim.Adam(this_RNN.parameters(), lr=learning_rate)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Training the model\n",
    "trained_RNN = train_rnn(this_RNN, batch_size, optimizer, criterion, num_epochs, show_every_n_batches)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A loss of less than 4 is sufficient for the purpose of generating fashion reviews, as we will see. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate fashion reviews "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now want to use our trained RNN to generate fashion reviews. The following function takes in the trained model, a \"start word\", our pre-processed data and the number of words to predict to produce a new review. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(rnn, start_word_id, int_to_vocab, token_dict, pad_value, predict_len=100):\n",
    "    \n",
    "    # Put the network in evaluation mode\n",
    "    rnn.eval()\n",
    "    \n",
    "    # Create an empty sequence with pad_value and batch size 1\n",
    "    current_seq = np.full((1, sequence_length), pad_value)\n",
    "    \n",
    "    # The predicted sequence starts with the start_word (!)\n",
    "    current_seq[-1][-1] = start_word_id\n",
    "    predicted = [int_to_vocab[start_word_id]]\n",
    "\n",
    "    for _ in range(predict_len):\n",
    "\n",
    "        if train_on_gpu:\n",
    "            current_seq = torch.LongTensor(current_seq).cuda()\n",
    "        else:\n",
    "            current_seq = torch.LongTensor(current_seq)\n",
    "        \n",
    "        # Initialize the hidden state\n",
    "        hidden = rnn.init_hidden(current_seq.size(0))\n",
    "        \n",
    "        # Get the output of the rnn by providing a sequence\n",
    "        output, _ = rnn(current_seq, hidden)\n",
    "        \n",
    "        # Get the next word probabilities using Softmax\n",
    "        p = F.softmax(output, dim=1).data\n",
    "        if(train_on_gpu):\n",
    "            p = p.cpu() # move to cpu \n",
    "         \n",
    "        # Use top_k sampling to get the index of the next  \n",
    "        # most probable 10 words\n",
    "        top_k = 10\n",
    "        p, top_i = p.topk(top_k)\n",
    "        top_i = top_i.numpy().squeeze()\n",
    "        \n",
    "        # Pick one of these five words randomly as the next word\n",
    "        p = p.numpy().squeeze()\n",
    "        word_i = np.random.choice(top_i, p=p/p.sum())\n",
    "        \n",
    "        # Retrieve that word from the dictionary \n",
    "        word = int_to_vocab[word_i]\n",
    "        predicted.append(word)    \n",
    "        \n",
    "        # Move tensor back to CPU for Numpy\n",
    "        current_seq = current_seq.to(torch.device(\"cpu\"))\n",
    "        current_seq = np.roll(current_seq, -1, 1)\n",
    "        current_seq[-1][-1] = word_i\n",
    "    \n",
    "    generated_review = ' '.join(predicted)\n",
    "    \n",
    "    # Replace punctuation tokens\n",
    "    for key, token in token_dict.items():\n",
    "        ending = ' ' if key in ['\\n', '(', '\"'] else ''\n",
    "        generated_review = generated_review.replace(' ' + token.lower(), key)\n",
    "    generated_review = generated_review.replace('\\n ', '\\n')\n",
    "    generated_review = generated_review.replace('( ', '(')\n",
    "    \n",
    "    # Return generated review\n",
    "    return generated_review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_new_review(model):\n",
    "    \n",
    "    start_word = input('Enter start word:') # is the word at the beginning of the script\n",
    "    start_word = start_word.lower() # as the vocab_to_int is in all lower case\n",
    "\n",
    "    review_length = int(input('Enter the length of a new review (number of words):')) # this specifies the number of words in the generated review\n",
    "    \n",
    "    # Generating 3 random reviews\n",
    "    print('\\nGenerating random reviews:')\n",
    "    for idx in range(3):\n",
    "        new_review = generate(model, vocab_to_int[start_word], \n",
    "                          int_to_vocab, token_dict, vocab_to_int[padding_words['PADDING']], review_length)\n",
    "        print('Generated review:\\n', new_review, '\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter start word:the\n",
      "Enter the length of a new review (number of words):75\n",
      "\n",
      "Generating random reviews:\n",
      "Generated review:\n",
      " the length, i bought both colors. it's a great dress but i love the fit in the front, which is a little more relaxed. the colors are beautiful and the fabric is a lovely and soft detail. i bought this dress today and it was great. this top is super comfortable and comfortable and it is a great piece for fall, and the fabric is a great addition to \n",
      "\n",
      "Generated review:\n",
      " the color. i got the pink and it has a nice touch of colors. it's not a dress for a wedding. i love the fabric and the fit, but i didn't find the skirt for a more comfortable fit. i was swimming in this top so much that you don't find the dress too tight in it to have a small, but i didn't need the small to be a \n",
      "\n",
      "Generated review:\n",
      " the fabric and the fit. i am 5\" and it fits well. i am usually a 2 but bought the small, and i ordered this in my usual size s which fits great on my frame. the fabric is light and airy. it is very flattering, and is very flattering and i am a 36c and the large fit well. if you are busty, i am not \n",
      "\n"
     ]
    }
   ],
   "source": [
    "generate_new_review(trained_RNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving  the model\n",
    "We are saving the model so that we can come back at a later date for future inference. This involves saving:\n",
    "- model architechture parameters \n",
    "- parameters of the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving model parameters and state_dict\n",
    "save_checkpoint ={'vocab_size': vocab_size,\n",
    "                  'output_size': output_size,\n",
    "                  'embedding_dim': embedding_dim,\n",
    "                  'hidden_dim': hidden_dim,\n",
    "                  'n_layers': n_layers,\n",
    "                  'dropout': 0.5,\n",
    "                  'learning_rate': learning_rate,\n",
    "                  'model_state_dict': trained_RNN.state_dict(),\n",
    "                  'optimizer_state_dict': optimizer.state_dict()\n",
    "                 }\n",
    "\n",
    "torch.save(save_checkpoint, 'trained_fashion.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the model and setting parameters\n",
    "We can re-load this saved model for futher \"play\" with generating fashion reviews. We need to ensure the following parameters are set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-processed vocabularty\n",
    "preprocess_filename = 'preprocess.p'\n",
    "int_reviews, vocab_to_int, int_to_vocab, token_dict = load_preprocess(preprocess_filename)\n",
    "\n",
    "padding_words = {'PADDING': '<PAD>'}\n",
    "\n",
    "sequence_length = 25 # Setting to a sequence length of 25 (roughly the length of a sentence)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also need to run the following further up in this Notebook to ensure our saved model will load and run properly. \n",
    "- class RNN \n",
    "- def generate\n",
    "- def generate_new_review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RNN(\n",
       "  (embedding): Embedding(17487, 437)\n",
       "  (lstm): LSTM(437, 437, num_layers=2, batch_first=True)\n",
       "  (dropout): Dropout(p=0.5)\n",
       "  (fc): Linear(in_features=437, out_features=17487, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading model parameters and state_dicts\n",
    "load_checkpoint = torch.load('trained_fashion.pt')\n",
    "\n",
    "# Re-create model\n",
    "loaded_rnn =  RNN(load_checkpoint['vocab_size'],\n",
    "                  load_checkpoint['output_size'],\n",
    "                  load_checkpoint['embedding_dim'],\n",
    "                  load_checkpoint['hidden_dim'],\n",
    "                  load_checkpoint['n_layers'],\n",
    "                  load_checkpoint['dropout'])\n",
    "\n",
    "# Load model state_dict\n",
    "loaded_rnn.load_state_dict(load_checkpoint['model_state_dict'])\n",
    "\n",
    "# Specify optimizer and state_dict\n",
    "optimizer = torch.optim.Adam(loaded_rnn.parameters(), lr=load_checkpoint['learning_rate'])\n",
    "optimizer.load_state_dict(load_checkpoint['optimizer_state_dict'])\n",
    "\n",
    "# Move model to GPU\n",
    "if train_on_gpu:\n",
    "    loaded_rnn.cuda()\n",
    "\n",
    "loaded_rnn.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter start word:that\n",
      "Enter the length of a new review (number of words):100\n",
      "\n",
      "Generating random reviews:\n",
      "Generated review:\n",
      " that i would have bought the white, and i love the style. the fabric is beautiful and the material is very soft but it makes it perfect. the fabric is very soft and a little more like it is not the perfect sweater. i ordered a medium but it fits just right. i am 5'8\" and i have a petite chest, which is so i have broad shoulders and this dress looks perfect. it is so comfortable but looks great.\n",
      "the fit is great on me! it is a great \n",
      "\n",
      "Generated review:\n",
      " that i have a larger bust. i have a short torso and the xs was perfect!\n",
      "\n",
      "the color is very soft, and it is very cute and a little too tight. it's so pretty. i got the pink, and it looked a tad more versatile. i think you can pair it with a jacket or dress or dress for work.\n",
      "- i would have purchased it online, but i am not sure if i had to get a small(and i got it in the red(and \n",
      "\n",
      "Generated review:\n",
      " that i was worried it was too small in the top and fit perfectly. i like the length of the top, and the dress fit nicely on me. i have the other reviews and i bought this shirt on sale and was worried it will be a little more casual, but i like a dress. love this top! i think it is not itchy, but i like the fabric on the fabric, and i think this top is a cute. the only thing i have, and i would have purchased it \n",
      "\n"
     ]
    }
   ],
   "source": [
    "generate_new_review(loaded_rnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p36",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
